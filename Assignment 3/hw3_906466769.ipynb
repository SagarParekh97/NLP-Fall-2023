{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from copy import copy\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LEN = 256\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "LR = 2e-5\n",
    "TRAINING_BATCH_SIZE = 16\n",
    "VAL_BATCH_SIZE = 1\n",
    "EPOCHS = 4\n",
    "DROPOUT = 0.3\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 25262\n",
      "Test dataset size: 6963\n",
      "Dev dataset size: 2985\n"
     ]
    }
   ],
   "source": [
    "loc = './dataset/'\n",
    "\n",
    "train = pd.read_csv(loc + 'train.csv')\n",
    "test = pd.read_json(loc + 'test.jsonl', lines=True)\n",
    "val = pd.read_csv(loc + 'valid.csv')\n",
    "\n",
    "print('Train dataset size: {}'.format(len(train)))\n",
    "print('Test dataset size: {}'.format(len(test)))\n",
    "print('Dev dataset size: {}'.format(len(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define collate function for dataloader**\n",
    "\n",
    "Need to define how to stack batches since different sentences can have different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_device(*args):\n",
    "    return (item.to(DEVICE) \n",
    "            if isinstance(item, torch.Tensor) \n",
    "            else item\n",
    "            for item in args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fcn(batch):\n",
    "    ids = [x['id'] for x in batch]\n",
    "    features = [x['features'] for x in batch]\n",
    "    tokens_batch, input_ids_batch, input_masks_batch, token_type_ids_batch, p_len_batch, q_len_batch, a_len_batch = ([] for _ in range(7))\n",
    "    for f_i in features:\n",
    "        tokens, input_ids, input_masks, token_type_ids, p_len, q_len, a_len = ([] for _ in range(7))\n",
    "        for f in f_i:\n",
    "            tokens.append(f[0])\n",
    "            input_ids.append(f[1])\n",
    "            input_masks.append(f[2])\n",
    "            token_type_ids.append(f[3])\n",
    "            p_len.append(f[4])\n",
    "            q_len.append(f[5])\n",
    "            a_len.append(f[6])\n",
    "        tokens_batch.append(tokens)\n",
    "        input_ids_batch.append(input_ids)\n",
    "        input_masks_batch.append(input_masks)\n",
    "        token_type_ids_batch.append(token_type_ids)\n",
    "        p_len_batch.append(p_len)\n",
    "        q_len_batch.append(q_len)\n",
    "        a_len_batch.append(a_len)\n",
    "    input_ids_batch = torch.tensor(input_ids_batch, dtype=torch.long)\n",
    "    input_masks_batch = torch.tensor(input_masks_batch, dtype=torch.long)\n",
    "    token_type_ids_batch = torch.tensor(token_type_ids_batch, dtype=torch.long)\n",
    "    p_len_batch = torch.tensor(p_len_batch, dtype=torch.long)\n",
    "    q_len_batch = torch.tensor(q_len_batch, dtype=torch.long)\n",
    "    a_len_batch = torch.tensor(a_len_batch, dtype=torch.long)\n",
    "    labels = torch.tensor([x['label'] for x in batch], dtype=torch.long)\n",
    "\n",
    "    return ids, tokens_batch, input_ids_batch, input_masks_batch, token_type_ids_batch, p_len_batch, q_len_batch, a_len_batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, add_CLS=True):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.add_CLS = add_CLS\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _truncate_seq(self, seq_1, seq_2, max_length):\n",
    "        ''' Truncate a sequence pair in place to keep the combined length = maximum length. \n",
    "            Always truncate the longer sequence. '''\n",
    "        while True:\n",
    "            total_len = len(seq_1) + len(seq_2)\n",
    "            if total_len <= max_length:\n",
    "                break\n",
    "            if len(seq_1) > len(seq_2):\n",
    "                seq_1.pop()\n",
    "            else:\n",
    "                seq_2.pop()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        id = row['id']\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "        options = [row['answer0'], row['answer1'], row['answer2'], row['answer3']]\n",
    "        correct_opt = row['label']\n",
    "        \n",
    "        # take care of the case where the option is None\n",
    "        options = [str(opt) if not isinstance(opt, str) else opt for opt in options]\n",
    "\n",
    "        feature_set = []\n",
    "        context_tokens = self.tokenizer.tokenize(context)\n",
    "        question_tokens = self.tokenizer.tokenize(question)\n",
    "        for opt in options:\n",
    "            opt_tokens = self.tokenizer.tokenize(opt)\n",
    "            q_opt_tok = question_tokens + opt_tokens\n",
    "\n",
    "            # truncate the context and question + option if they are too long\n",
    "            self._truncate_seq(context_tokens, q_opt_tok, self.max_len - 3)\n",
    "\n",
    "            ''' [CLS] context [SEP] question option [SEP] '''\n",
    "            ''' [ 0      0      0      1       1      1 ] '''\n",
    "            tokens = ['[CLS]'] + context_tokens + ['[SEP]'] + q_opt_tok + ['[SEP]']\n",
    "            token_type_ids = [0] * (len(context_tokens) + 2) + [1] * (len(q_opt_tok) + 1)\n",
    "            \n",
    "            input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            input_masks = [1] * len(input_ids)\n",
    "\n",
    "            # pad the tokens to max length\n",
    "            padding_length = self.max_len - len(input_ids)\n",
    "            padding = [0] * padding_length\n",
    "            input_ids += padding\n",
    "            input_masks += padding\n",
    "            token_type_ids += padding\n",
    "\n",
    "            assert len(input_ids) == self.max_len, \"Input ids should be {}, but is {} instead\".format(self.max_len, len(input_ids))\n",
    "            assert len(input_masks) == self.max_len, \"Input masks should be {}, but is {} instead\".format(self.max_len, len(input_masks))\n",
    "            assert len(token_type_ids) == self.max_len, \"Token type ids should be {}, but is {} instead\".format(self.max_len, len(token_type_ids))\n",
    "\n",
    "            feature_set.append((tokens, input_ids, input_masks, token_type_ids,\n",
    "                                len(context_tokens), len(question_tokens), len(opt_tokens)))\n",
    "\n",
    "        return {'id': id,\n",
    "                'features': feature_set,\n",
    "                'label': correct_opt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture for Question-Answering\n",
    "To enhance the context understanding ability of BERT fine-tuning, we perform multiway bidirectional attention over the BERT encoding output, refer to **COSMOS QA: Machine Reading Comprehension with Contextual Commonsense Reasoning** [(arxiv)](https://arxiv.org/pdf/1909.00277.pdf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_multChoice(nn.Module):\n",
    "    def __init__(self, dropout, hidden_size=None):\n",
    "        super(BERT_multChoice, self).__init__()\n",
    "\n",
    "        self.bert_encoder = BertModel.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        if hidden_size is None:\n",
    "            hidden_size = self.bert_encoder.config.hidden_size\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # self.linear_tran = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.linear_fuseP = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        # self.linear_fuseQ = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        # self.linear_fuseA = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        # self.classifier = nn.Linear(hidden_size*3, NUM_CLASSES)\n",
    "        self.classifier = nn.Linear(hidden_size, NUM_CLASSES)\n",
    "        self.loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, p_len, q_len, a_len, labels):\n",
    "        # flatten the inputs\n",
    "        flat_input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.shape[-1])\n",
    "        flat_attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "        \n",
    "        output = self.bert_encoder(input_ids=flat_input_ids,\n",
    "                                             token_type_ids=flat_token_type_ids,\n",
    "                                             attention_mask=flat_attention_mask)\n",
    "        pooled_output = output[1]\n",
    "\n",
    "        # reshape the outputs\n",
    "        pooled_output = pooled_output.view(-1, 4, pooled_output.shape[-1])\n",
    "        pooled_output = torch.sum(pooled_output, dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        logits = self.classifier(pooled_output)\n",
    "        reshaped_logits = logits.view(-1, NUM_CLASSES)\n",
    "\n",
    "        loss = self.loss_fcn(reshaped_logits, labels)\n",
    "        prediction = torch.argmax(reshaped_logits, dim=1)\n",
    "\n",
    "        return loss, prediction\n",
    "    \n",
    "    def loss(self, label, prediction):\n",
    "        return self.loss_fcn(prediction, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, optimizer, train_loader, val_loader):\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    training_loss = []\n",
    "    training_accuracy = []\n",
    "    validation_loss = []\n",
    "    validation_accuracy = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        correct_predictions = 0\n",
    "        train_sample_count = 0\n",
    "\n",
    "        ''' training '''\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader, desc='Training', ncols=100, leave=False):\n",
    "            ids, tokens_batch, input_ids_batch, input_masks_batch, token_type_ids_batch, \\\n",
    "                p_len_batch, q_len_batch, a_len_batch, labels = send_to_device(*batch)\n",
    "\n",
    "            loss, prediction = model(input_ids_batch, token_type_ids_batch, input_masks_batch, \n",
    "                                p_len_batch, q_len_batch, a_len_batch, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            correct_predictions += torch.sum(prediction == labels).cpu().detach().numpy()\n",
    "            training_loss.append(loss.item())\n",
    "            train_sample_count += len(batch)\n",
    "\n",
    "        training_accuracy.append((epoch, correct_predictions / train_sample_count))\n",
    "\n",
    "        ''' validation '''\n",
    "        val_sample_count = 0\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        model.eval()\n",
    "        for data in tqdm(val_loader, desc='Validation', ncols=100, leave=False):\n",
    "            ids, tokens_batch, input_ids_batch, input_masks_batch, token_type_ids_batch, \\\n",
    "                p_len_batch, q_len_batch, a_len_batch, labels = data\n",
    "\n",
    "            loss, prediction = model(input_ids_batch, token_type_ids_batch, input_masks_batch, \n",
    "                                     p_len_batch, q_len_batch, a_len_batch, labels)\n",
    "            \n",
    "            validation_loss.append(loss.item())\n",
    "            correct_predictions += torch.sum(prediction == labels).cpu().detach().numpy()\n",
    "            val_sample_count += len(data)\n",
    "            \n",
    "        validation_accuracy.append((epoch, correct_predictions / val_sample_count))    \n",
    "\n",
    "        tqdm.write(f'Epoch: {epoch}, Training loss: {np.mean(training_loss)}, Training accuracy: {sum(list(zip(*training_accuracy))[1])},' \\\n",
    "                   f' Validation loss: {np.mean(validation_loss)}, Validation accuracy: {sum(list(zip(*validation_accuracy))[1])}')\n",
    "\n",
    "    # save the trained models\n",
    "    model_checkpoint = dict()\n",
    "    model_checkpoint['model_state_dict'] = model.state_dict()\n",
    "    model_checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
    "    model_checkpoint['training_loss'] = training_loss\n",
    "    model_checkpoint['training_accuracy'] = training_accuracy\n",
    "    model_checkpoint['validation_loss'] = validation_loss\n",
    "    model_checkpoint['validation_accuracy'] = validation_accuracy\n",
    "    torch.save(model_checkpoint, f'./save_data/model_checkpoint.pth')\n",
    "    \n",
    "    \n",
    "    return training_loss, training_accuracy, validation_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train, tokenizer, MAX_LEN)\n",
    "val_dataset = Dataset(val, tokenizer, MAX_LEN)\n",
    "test_dataset = Dataset(test, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dataLoader = {'batch_size': TRAINING_BATCH_SIZE,\n",
    "                     'shuffle': True,\n",
    "                     'num_workers': NUM_WORKERS,\n",
    "                     'collate_fn': collate_fcn}\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset, **params_dataLoader)\n",
    "\n",
    "params_dataLoader = {'batch_size': VAL_BATCH_SIZE,\n",
    "                     'shuffle': True,\n",
    "                     'num_workers': 0,\n",
    "                     'collate_fn': collate_fcn}\n",
    "val_dataset_loader = torch.utils.data.DataLoader(val_dataset, **params_dataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_multChoice(dropout=DROPOUT).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sagar-legion/Projects/NLP-Fall-2023/Assignment 3/hw3_906466769.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%203/hw3_906466769.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%203/hw3_906466769.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(\u001b[39m'\u001b[39m\u001b[39msave_data\u001b[39m\u001b[39m'\u001b[39m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%203/hw3_906466769.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     training_loss, training_accuracy, validation_loss, validation_accuracy \u001b[39m=\u001b[39m trainer(model, optimizer, train_dataset_loader, val_dataset_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%203/hw3_906466769.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     fname \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./save_data/training_loss.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%203/hw3_906466769.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(fname, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;32m/home/sagar-legion/Projects/NLP-Fall-2023/Assignment 3/hw3_906466769.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%203/hw3_906466769.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m loss, prediction \u001b[39m=\u001b[39m model(input_ids_batch, token_type_ids_batch, input_masks_batch, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%203/hw3_906466769.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m                     p_len_batch, q_len_batch, a_len_batch, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%203/hw3_906466769.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%203/hw3_906466769.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%203/hw3_906466769.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%203/hw3_906466769.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m correct_predictions \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(prediction \u001b[39m==\u001b[39m labels)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_hw1/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_hw1/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load = all([os.path.exists(f'save_data/{f}') for f in ['model_checkpoint.pth', 'training_loss.json', 'training_accuracy.json',\n",
    "                                                       'validation_loss.json', 'validation_accuracy.json']])\n",
    "\n",
    "if load:\n",
    "    model_dir = 'save_data/model_checkpoint.pth'\n",
    "    model_checkpoint = torch.load(model_dir)\n",
    "    model.load_state_dict(model_checkpoint['model_state_dict'])\n",
    "\n",
    "    fname = os.path.join(f'./save_data/training_loss.json')\n",
    "    with open(fname, 'r') as f:\n",
    "        training_loss = json.load(f)\n",
    "    fname = os.path.join(f'./save_data/training_accuracy.json')\n",
    "    with open(fname, 'r') as f:\n",
    "        training_accuracy = json.load(f)\n",
    "    fname = os.path.join(f'./save_data/validation_loss.json')\n",
    "    with open(fname, 'r') as f:\n",
    "        validation_loss = json.load(f)\n",
    "    fname = os.path.join(f'./save_data/validation_accuracy.json')\n",
    "    with open(fname, 'r') as f:\n",
    "        validation_accuracy = json.load(f)\n",
    "else:\n",
    "    os.makedirs('save_data', exist_ok=True)\n",
    "    training_loss, training_accuracy, validation_loss, validation_accuracy = trainer(model, optimizer, train_dataset_loader, val_dataset_loader)\n",
    "\n",
    "    fname = os.path.join(f'./save_data/training_loss.json')\n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(training_loss, f)\n",
    "    fname = os.path.join(f'./save_data/training_accuracy.json')\n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(training_accuracy, f)\n",
    "    fname = os.path.join(f'./save_data/validation_loss.json')\n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(validation_loss, f)\n",
    "    fname = os.path.join(f'./save_data/validation_accuracy.json')\n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(validation_accuracy, f)\n",
    "\n",
    "_, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "ax[0, 0].plot(training_loss, marker='.')\n",
    "ax[0, 0].set_title('Training Loss')\n",
    "ax[0, 1].plot(*zip(*training_accuracy))\n",
    "ax[0, 1].set_title('Training Accuracy')\n",
    "ax[1, 0].plot(validation_loss, marker='.')\n",
    "ax[1, 0].set_title('Validation Loss')\n",
    "ax[1, 1].plot(*zip(*validation_accuracy))\n",
    "ax[1, 1].set_title('Validation Accuracy')\n",
    "\n",
    "ax[0, 1].set_ylim([0, 1])\n",
    "ax[1, 1].set_ylim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
