{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from copy import copy\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LEN = 256\n",
    "\n",
    "LR = 2e-5\n",
    "TRAINING_BATCH_SIZE = 16\n",
    "VAL_BATCH_SIZE = 1\n",
    "EPOCHS = 4\n",
    "DROPOUT = 0.3\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = './dataset/'\n",
    "\n",
    "train = pd.read_csv(loc + 'train.csv')\n",
    "test = pd.read_json(loc + 'test.jsonl', lines=True)\n",
    "val = pd.read_csv(loc + 'valid.csv')\n",
    "\n",
    "print('Train dataset size: {}'.format(len(train)))\n",
    "print('Test dataset size: {}'.format(len(test)))\n",
    "print('Dev dataset size: {}'.format(len(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define collate function for dataloader**\n",
    "\n",
    "Need to define how to stack batches since different sentences can have different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fcn(batch):\n",
    "    ids = [x['id'] for x in batch]\n",
    "    features = [x['features'] for x in batch]\n",
    "    tokens_batch, input_ids_batch, input_masks_batch, token_type_ids_batch, p_len_batch, q_len_batch, a_len_batch = ([] for _ in range(7))\n",
    "    for f_i in features:\n",
    "        tokens, input_ids, input_masks, token_type_ids, p_len, q_len, a_len = ([] for _ in range(7))\n",
    "        for f in f_i:\n",
    "            tokens.append(f[0])\n",
    "            input_ids.append(f[1])\n",
    "            input_masks.append(f[2])\n",
    "            token_type_ids.append(f[3])\n",
    "            p_len.append(f[4])\n",
    "            q_len.append(f[5])\n",
    "            a_len.append(f[6])\n",
    "        tokens_batch.append(tokens)\n",
    "        input_ids_batch.append(input_ids)\n",
    "        input_masks_batch.append(input_masks)\n",
    "        token_type_ids_batch.append(token_type_ids)\n",
    "        p_len_batch.append(p_len)\n",
    "        q_len_batch.append(q_len)\n",
    "        a_len_batch.append(a_len)\n",
    "    input_ids_batch = torch.tensor(input_ids_batch, dtype=torch.long)\n",
    "    input_masks_batch = torch.tensor(input_masks_batch, dtype=torch.long)\n",
    "    token_type_ids_batch = torch.tensor(token_type_ids_batch, dtype=torch.long)\n",
    "    p_len_batch = torch.tensor(p_len_batch, dtype=torch.long)\n",
    "    q_len_batch = torch.tensor(q_len_batch, dtype=torch.long)\n",
    "    a_len_batch = torch.tensor(a_len_batch, dtype=torch.long)\n",
    "    labels = torch.tensor([x['label'] for x in batch], dtype=torch.long)\n",
    "\n",
    "    return ids, tokens_batch, input_ids_batch, input_masks_batch, token_type_ids_batch, p_len_batch, q_len_batch, a_len_batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, add_CLS=True):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.add_CLS = add_CLS\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        id = row['id']\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "        options = [row['answer0'], row['answer1'], row['answer2'], row['answer3']]\n",
    "        correct_opt = row['label']\n",
    "        \n",
    "        feature_set = []\n",
    "        context_tokens = self.tokenizer.tokenize(context)\n",
    "        question_tokens = self.tokenizer.tokenize(question)\n",
    "        for opt in options:\n",
    "            opt_tokens = self.tokenizer.tokenize(opt)\n",
    "            q_opt_tok = question_tokens + opt_tokens\n",
    "\n",
    "            ''' [CLS] context [SEP] question option [SEP] '''\n",
    "            ''' [ 0      0      0      1       1      1 ] '''\n",
    "            tokens = ['[CLS]'] + context_tokens + ['[SEP]'] + q_opt_tok + ['[SEP]']\n",
    "            token_type_ids = [0] * (len(context_tokens) + 2) + [1] * (len(q_opt_tok) + 1)\n",
    "            \n",
    "            input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            input_masks = [1] * len(input_ids)\n",
    "\n",
    "            # pad the tokens to max length\n",
    "            padding_length = self.max_len - len(input_ids)\n",
    "            padding = [0] * padding_length\n",
    "            input_ids += padding\n",
    "            input_masks += padding\n",
    "            token_type_ids += padding\n",
    "\n",
    "            assert len(input_ids) == self.max_len\n",
    "            assert len(input_masks) == self.max_len\n",
    "            assert len(token_type_ids) == self.max_len\n",
    "\n",
    "            feature_set.append((tokens, input_ids, input_masks, token_type_ids,\n",
    "                                len(context_tokens), len(question_tokens), len(opt_tokens)))\n",
    "\n",
    "        return {'id': id,\n",
    "                'features': feature_set,\n",
    "                'label': correct_opt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture for Question-Answering\n",
    "To enhance the context understanding ability of BERT fine-tuning, we perform multiway bidirectional attention over the BERT encoding output, refer to **COSMOS QA: Machine Reading Comprehension with Contextual Commonsense Reasoning** [(arxiv)](https://arxiv.org/pdf/1909.00277.pdf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_multAttn(nn.Module):\n",
    "    def __init__(self, dropout, hidden_size):\n",
    "        super(BERT_multAttn, self).__init__()\n",
    "\n",
    "        self.bert_encoder = BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_tran = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_fuseP = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.linear_fuseQ = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.linear_fuseA = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.classifier = nn.Linear(hidden_size*3, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, p_len, q_len, a_len, labels):\n",
    "        # flatten the inputs\n",
    "        flat_input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.shape[-1])\n",
    "        flat_attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n",
    "        \n",
    "        sequence_output, pooled_output = self.bert_encoder(input_ids=flat_input_ids,\n",
    "                                                           token_type_ids=flat_token_type_ids,\n",
    "                                                           attention_mask=flat_attention_mask,\n",
    "                                                           output_all_encoded_layers=False)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the pretrained tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train, tokenizer, MAX_LEN)\n",
    "val_dataset = Dataset(val, tokenizer, MAX_LEN)\n",
    "test_dataset = Dataset(test, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dataLoader = {'batch_size': TRAINING_BATCH_SIZE,\n",
    "                     'shuffle': True,\n",
    "                     'num_workers': NUM_WORKERS,\n",
    "                     'collate_fn': collate_fcn}\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset, **params_dataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "torch.Size([16, 4, 256])\n",
      "torch.Size([16, 4, 256])\n",
      "torch.Size([16, 4, 256])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dfsd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(a_len_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mdfsd\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfsd' is not defined"
     ]
    }
   ],
   "source": [
    "for batch in train_dataset_loader:\n",
    "    ids, tokens_batch, input_ids_batch, input_masks_batch, token_type_ids_batch, p_len_batch, q_len_batch, a_len_batch, labels = batch\n",
    "    print(len(ids))\n",
    "    print(len(tokens_batch))\n",
    "    print(input_ids_batch.shape)\n",
    "    print(input_masks_batch.shape)\n",
    "    print(token_type_ids_batch.shape)\n",
    "    print(p_len_batch.shape)\n",
    "    print(q_len_batch.shape)\n",
    "    print(a_len_batch.shape)\n",
    "    print(labels.shape)\n",
    "    dfsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
