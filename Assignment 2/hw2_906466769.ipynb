{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from copy import copy\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read all relation labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 30 relations:\n",
      "['country', 'capital', 'administrative_divisions', 'neighborhood_of', 'contains', 'nationality', 'place_lived', 'place_of_death', 'company', 'capital', 'place_of_birth', 'children', 'founders', 'place_founded', 'location', 'ethnicity', 'geographic_distribution', 'religion', 'major_shareholders', 'capital', 'capital', 'advisors', 'featured_in_films', 'featured_film_locations', 'county_seat', 'locations', 'place_of_burial', 'interred_here', 'companies_advised', 'other']\n"
     ]
    }
   ],
   "source": [
    "loc = './dataset/NYT29/relations.txt'\n",
    "RELATION_LABELS = [f.split('/')[-1] for f in open(loc, 'r').read().splitlines()]\n",
    "RELATION_LABELS.append('other')\n",
    "\n",
    "print('Dataset contains {} relations:'.format(len(RELATION_LABELS)))\n",
    "print(RELATION_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(RELATION_LABELS)\n",
    "MODEL_NAME = 'roberta-base'\n",
    "MAX_LEN = 128\n",
    "\n",
    "LR = 2e-5\n",
    "TRAINING_BATCH_SIZE = 16\n",
    "VAL_BATCH_SIZE = 1\n",
    "EPOCHS = 4\n",
    "DROPOUT = 0.3\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define dataset class for processing inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entity(entity_list):\n",
    "    if not isinstance(entity_list, list):\n",
    "        entity_list = [entity_list]\n",
    "    # remove white spaces and newline characters\n",
    "    for idx, e in enumerate(entity_list):\n",
    "        e = e.strip()\n",
    "        entity_list[idx] = e\n",
    "    return entity_list\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dict, tokenizer, max_len, add_CLS=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.sentences = dataset_dict['sent']\n",
    "        self.relation_tuples = dataset_dict['tup']\n",
    "        if len(self.sentences) != len(self.relation_tuples):\n",
    "            raise ValueError('The number of sentences and relation tuples are not equal.')\n",
    "        self.add_CLS = add_CLS\n",
    "        \n",
    "        self.separator = '|'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get the sentence and the corresponding relation tuple\n",
    "        sentence = self.sentences[index]\n",
    "        relation_tuples = self.relation_tuples[index]\n",
    "        relation_tuples = relation_tuples.split(self.separator)\n",
    "\n",
    "        # find all entities mentioned\n",
    "        all_word_pairs_mentioned = [clean_entity(x.split(';')[:2]) for x in relation_tuples]\n",
    "        all_entity_pairs_mentioned = [clean_entity(x.split(';')[-1].split('/')[1:3]) for x in relation_tuples]\n",
    "        all_relation_labels = [clean_entity(x.split(';')[-1].split('/')[-1])[0] for x in relation_tuples]\n",
    "        all_entities = [item for sublist in all_entity_pairs_mentioned for item in sublist]\n",
    "        all_entities = list(set(all_entities))\n",
    "        all_possible_entity_pairs = [[x, y] for x in all_entities for y in all_entities if x != y]\n",
    "\n",
    "        # look at all possible entity pairs; positive examples: if entity pair is mentioned, negative examples: if entity pair is not mentioned\n",
    "        # replace the word in the sentence with its NER tag and tokenize the masked sentence\n",
    "        # save the tokenized sentences for each entity mention and the corresponding relation label\n",
    "        output = {'masked sentences': [],\n",
    "                  'input_ids': [],\n",
    "                  'attention_masks': [],\n",
    "                  'target_label_ids': [],\n",
    "                  'entity_pairs': []}\n",
    "        \n",
    "        for entity_pair in all_possible_entity_pairs:\n",
    "            ''' positive examples with given relations '''\n",
    "            if entity_pair in all_entity_pairs_mentioned:\n",
    "                sentence_copy = copy(sentence)\n",
    "                idx = all_entity_pairs_mentioned.index(entity_pair)\n",
    "                word_pair = all_word_pairs_mentioned[idx]\n",
    "\n",
    "                word1, word2 = word_pair\n",
    "                entity1, entity2 = entity_pair\n",
    "                relation_label = all_relation_labels[idx]\n",
    "                relation_label = relation_label\n",
    "\n",
    "                masked_sentence = sentence_copy.replace(word1, entity1)\n",
    "                masked_sentence = masked_sentence.replace(word2, entity2)\n",
    "            else:\n",
    "                ''' negative examples with \"other\" relation '''\n",
    "                sentence_copy = copy(sentence)\n",
    "\n",
    "                entity1, entity2 = entity_pair\n",
    "                # find corresponding word\n",
    "                for idx1, e in enumerate(all_entity_pairs_mentioned):\n",
    "                    try:\n",
    "                        idx2 = e.index(entity1)\n",
    "                        word1 = all_word_pairs_mentioned[idx1][idx2]\n",
    "                    except:\n",
    "                        pass\n",
    "                    try:\n",
    "                        idx3 = e.index(entity2)\n",
    "                        word2 = all_word_pairs_mentioned[idx1][idx3]\n",
    "                    except:\n",
    "                        continue\n",
    "                relation_label = 'other'\n",
    "\n",
    "                masked_sentence = sentence_copy.replace(word1, entity1)\n",
    "                masked_sentence = masked_sentence.replace(word2, entity2)\n",
    "\n",
    "            # tokenize the masked sentence\n",
    "            encoding = self.tokenizer(masked_sentence, max_length=self.max_len, padding='max_length', \n",
    "                                      return_attention_mask=True, truncation=True, add_special_tokens=self.add_CLS)\n",
    "            input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n",
    "            attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n",
    "            target_label_id = torch.tensor(RELATION_LABELS.index(relation_label), dtype=torch.long)\n",
    "\n",
    "            output['masked sentences'].append(masked_sentence)\n",
    "            output['input_ids'].append(input_ids)\n",
    "            output['entity_pairs'].append(entity_pair)\n",
    "            output['attention_masks'].append(attention_mask)\n",
    "            output['target_label_ids'].append(target_label_id)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dropout=0.3, num_classes=30):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "\n",
    "        self.loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, ids, masks, targets):\n",
    "        outputs = self.roberta(ids, attention_mask=masks)\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]\n",
    "        context = self.dropout(pooled_output)\n",
    "\n",
    "        logits = self.classifier(pooled_output)\n",
    "        _, prediction = torch.argmax(logits, dim=1)\n",
    "\n",
    "        loss = self.loss_fcn(logits, targets)\n",
    "        return loss, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = './dataset/NYT29/'\n",
    "\n",
    "file_types = ['.sent', '.tup']\n",
    "datasets = ['train', 'test', 'dev']\n",
    "\n",
    "train = {}\n",
    "test = {}\n",
    "dev = {}\n",
    "\n",
    "for d in datasets:\n",
    "    for t in file_types:\n",
    "        with open(os.path.join(dataset_dir, f'{d}{t}'), 'r') as f:\n",
    "            if t == '.sent':\n",
    "                exec(f'{d}[\"sent\"] = f.read().splitlines()')\n",
    "            else:\n",
    "                exec(f'{d}[\"tup\"] = f.read().splitlines()')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = Dataset(train, tokenizer, MAX_LEN)\n",
    "test_dataset = Dataset(test, tokenizer, MAX_LEN)\n",
    "dev_dataset = Dataset(dev, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, train_dataset_loader, dev_dataset_loader, optimizer, epochs, device):\n",
    "    \n",
    "\n",
    "    ''' train '''\n",
    "    training_loss = []\n",
    "    training_accuracy = []\n",
    "    for e in tqdm(range(epochs)):\n",
    "        correct_predictions = 0\n",
    "        for data in tqdm(train_dataset_loader):\n",
    "            ids = data['id'].to(device)\n",
    "            masks = data['mask'].to(device)\n",
    "            targets = data['target'].to(device)\n",
    "\n",
    "            loss, prediction = model(ids, masks, targets)\n",
    "            training_loss.append(loss.item())\n",
    "            correct_predictions += torch.sum(prediction == targets).cpu().detach().numpy()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        training_accuracy.append((e, correct_predictions / len(train_dataset)))\n",
    "\n",
    "        ''' validation '''\n",
    "        validation_loss = []\n",
    "        validation_accuracy = []\n",
    "        model.eval()\n",
    "        for data in tqdm(dev_dataset_loader):\n",
    "            ids = data['id'].to(device)\n",
    "            masks = data['mask'].to(device)\n",
    "            targets = data['target'].to(device)\n",
    "\n",
    "            loss, prediction = model(ids, masks, targets)\n",
    "            validation_loss.append(loss.item())\n",
    "            correct_predictions += torch.sum(prediction == targets).cpu().detach().numpy()\n",
    "        validation_accuracy.append((e, correct_predictions / len(dev_dataset)))    \n",
    "\n",
    "    # save the trained models\n",
    "    model_checkpoint = dict()\n",
    "    model_checkpoint['model_state_dict'] = model.state_dict()\n",
    "    model_checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
    "    model_checkpoint['training_loss'] = training_loss\n",
    "    model_checkpoint['training_accuracy'] = training_accuracy\n",
    "    model_checkpoint['validation_loss'] = validation_loss\n",
    "    model_checkpoint['validation_accuracy'] = validation_accuracy\n",
    "    torch.save(model_checkpoint, f'./save_data/model_checkpoint.pth')\n",
    "    return training_loss, training_accuracy, validation_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dataLoader = {'batch_size': TRAINING_BATCH_SIZE,\n",
    "                     'shuffle': True,\n",
    "                     'num_workers': NUM_WORKERS}\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset, **params_dataLoader)\n",
    "\n",
    "params_dataLoader_eval = {'batch_size': VAL_BATCH_SIZE,\n",
    "                          'shuffle': True,\n",
    "                          'num_workers': NUM_WORKERS}\n",
    "dev_dataset_loader = torch.utils.data.DataLoader(dev_dataset, **params_dataLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Classifier(dropout=DROPOUT, num_classes=NUM_CLASSES).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3957 [00:00<?, ?it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sagar-legion/Projects/NLP-Fall-2023/Assignment 2/hw2_906466769.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%202/hw2_906466769.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39m'\u001b[39m\u001b[39msave_data/\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%202/hw2_906466769.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(\u001b[39m'\u001b[39m\u001b[39msave_data\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%202/hw2_906466769.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m training_loss, training_accuracy, validation_loss, validation_accuracy \u001b[39m=\u001b[39m trainer(model, train_dataset_loader, dev_dataset_loader, optimizer, EPOCHS, device)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%202/hw2_906466769.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m fname \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./save_data/training_loss.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%202/hw2_906466769.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(fname, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;32m/home/sagar-legion/Projects/NLP-Fall-2023/Assignment 2/hw2_906466769.ipynb Cell 17\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%202/hw2_906466769.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%202/hw2_906466769.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     correct_predictions \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%202/hw2_906466769.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(train_dataset_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%202/hw2_906466769.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         ids \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/sagar-legion/Projects/NLP-Fall-2023/Assignment%202/hw2_906466769.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         masks \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_hw1/lib/python3.9/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_hw1/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_hw1/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_hw1/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_hw1/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_hw1/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:127\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m    126\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[1;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_hw1/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:127\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m    126\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[1;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_hw1/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:138\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    136\u001b[0m elem_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mnext\u001b[39m(it))\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(elem) \u001b[39m==\u001b[39m elem_size \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m it):\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "load = all([os.path.exists(f'save_data/{f}') for f in ['model_checkpoint.pth', 'training_loss.json', 'training_accuracy.json']])\n",
    "\n",
    "if load:\n",
    "    model_dir = 'save_data/model_checkpoint.pth'\n",
    "    model_checkpoint = torch.load(model_dir)\n",
    "    model.load_state_dict(model_checkpoint['model_state_dict'])\n",
    "\n",
    "    fname = os.path.join(f'./save_data/training_loss.json')\n",
    "    with open(fname, 'r') as f:\n",
    "        training_loss = json.load(f)\n",
    "    fname = os.path.join(f'./save_data/training_accuracy.json')\n",
    "    with open(fname, 'r') as f:\n",
    "        training_accuracy = json.load(f)\n",
    "    fname = os.path.join(f'./save_data/validation_loss.json')\n",
    "    with open(fname, 'r') as f:\n",
    "        validation_loss = json.load(f)\n",
    "    fname = os.path.join(f'./save_data/validation_accuracy.json')\n",
    "    with open(fname, 'r') as f:\n",
    "        validation_accuracy = json.load(f)\n",
    "else:\n",
    "    if not os.path.exists('save_data/'):\n",
    "        os.makedirs('save_data')\n",
    "    training_loss, training_accuracy, validation_loss, validation_accuracy = trainer(model, train_dataset_loader, dev_dataset_loader, optimizer, EPOCHS, device)\n",
    "\n",
    "    fname = os.path.join(f'./save_data/training_loss.json')\n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(training_loss, f)\n",
    "    fname = os.path.join(f'./save_data/training_accuracy.json')\n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(training_accuracy, f)\n",
    "    fname = os.path.join(f'./save_data/validation_loss.json')\n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(validation_loss, f)\n",
    "    fname = os.path.join(f'./save_data/validation_accuracy.json')\n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(validation_accuracy, f)\n",
    "\n",
    "_, ax = plt.subplots(2, 2, figsize=(10, 5))\n",
    "ax[0, 0].plot(training_loss, marker='.')\n",
    "ax[0, 0].set_title('Training Loss')\n",
    "ax[0, 1].plot(*zip(*training_accuracy))\n",
    "ax[0, 1].set_title('Training Accuracy')\n",
    "ax[1, 0].plot(validation_loss, marker='.')\n",
    "ax[1, 0].set_title('Validation Loss')\n",
    "ax[1, 1].plot(*zip(*validation_accuracy))\n",
    "ax[1, 1].set_title('Validation Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
